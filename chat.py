__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import asyncio
import streamlit as st
# Váº«n import cÃ¡c thÃ nh pháº§n tá»« langchain-core hoáº·c langchain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Thay Ä‘á»•i cÃ¡c import Ä‘á»ƒ trá» Ä‘áº¿n cÃ¡c gÃ³i cá»¥ thá»ƒ hÆ¡n
from langchain_community.document_loaders import WebBaseLoader # <-- THAY Äá»”I
from langchain_community.vectorstores import Chroma           # <-- THAY Äá»”I
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

import os
from dotenv import load_dotenv
load_dotenv()

GOOGLE_API_KEY = st.secrets["GOOGLE_GEMINI_API_KEY"]
WEBSITE_URL = "https://www.ceo.pro.vn/"

# --- HÃ€M Táº¢I Dá»® LIá»†U VÃ€ Xá»¬ LÃ ---
@st.cache_resource
def load_and_process_data(url, api_key):
    # Táº O VÃ€ SET EVENT LOOP Má»šI CHO LUá»’NG HIá»†N Táº I
    asyncio.set_event_loop(asyncio.new_event_loop()) # <-- THÃŠM DÃ’NG NÃ€Y

    loader = WebBaseLoader(url)
    data = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    chunks = text_splitter.split_documents(data)
    vector_store = Chroma.from_documents(
        chunks,
        embedding=GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    )
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    return retriever

# --- Táº O PROMPT TEMPLATE Äá»‚ SET ROLE ---
prompt_template = """Báº¡n lÃ  má»™t trá»£ lÃ½ AI há»¯u Ã­ch vÃ  thÃ¢n thiá»‡n, cÃ³ nhiá»‡m vá»¥ tráº£ lá»i cÃ¡c cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng chá»‰ dá»±a trÃªn thÃ´ng tin tá»« trang web Ä‘Æ°á»£c cung cáº¥p.
Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, má»™t cÃ¡ch rÃµ rÃ ng vÃ  chuyÃªn nghiá»‡p.

Ngá»¯ cáº£nh:
{context}

CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: {question}

CÃ¢u tráº£ lá»i cá»§a báº¡n:"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

# --- GIAO DIá»†N STREAMLIT ---
st.set_page_config(page_title="Chatbot Website vá»›i Gemini", page_icon="ğŸ¤–")
st.title("ğŸ¤– Chatbot Há»— Trá»£ ThÃ´ng Tin Website")
st.caption(f"TÃ´i lÃ  trá»£ lÃ½ áº£o, sáºµn sÃ ng tráº£ lá»i cÃ¡c cÃ¢u há»i tá»« trang: {WEBSITE_URL}")

try:
    retriever = load_and_process_data(WEBSITE_URL, GOOGLE_API_KEY)
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GOOGLE_API_KEY, temperature=0.7)
    
    chain_type_kwargs = {"prompt": PROMPT}
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs=chain_type_kwargs,
        return_source_documents=True
    )

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ChÃ o báº¡n! TÃ´i lÃ  trá»£ lÃ½ áº£o cá»§a trang web nÃ y. Báº¡n cáº§n tÃ¬m thÃ´ng tin gÃ¬ áº¡?"}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Báº¡n muá»‘n há»i gÃ¬ vá» ná»™i dung trang web?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Äang tÃ¬m kiáº¿m vÃ  tá»•ng há»£p cÃ¢u tráº£ lá»i..."):
                response = qa_chain(prompt)
                answer = response['result']
                st.markdown(answer)
        
        st.session_state.messages.append({"role": "assistant", "content": answer})

except Exception as e:
    st.error(f"ÄÃ£ xáº£y ra lá»—i: {e}")
    st.info("Vui lÃ²ng kiá»ƒm tra láº¡i Google API Key vÃ  URL cá»§a website.")
